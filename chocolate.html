<!DOCTYPE html>
<!--[if lt IE 7 ]><html class="ie ie6" lang="en"> <![endif]-->
<!--[if IE 7 ]><html class="ie ie7" lang="en"> <![endif]-->
<!--[if IE 8 ]><html class="ie ie8" lang="en"> <![endif]-->
<!--[if (gte IE 9)|!(IE)]><!--><html lang="en"> <!--<![endif]-->
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="author" content="Garry Chan">
  <meta name="description" content="Posts by Garry Chan">
  <title>
    github.com/garrrychan
&ndash; Life is like a box of chocolates  </title>

    <link rel="canonical" href="https://garrrychan.github.io/blog/chocolate.html">

  <link href="//fonts.googleapis.com/css?family=Open+Sans:600,800" rel="stylesheet" type="text/css">
  <!------>
  <link rel="shortcut icon" href="https://garrrychan.github.io/blog/theme/images/logo.png">
  <link rel="stylesheet" type="text/css" href="https://garrrychan.github.io/blog/theme/css/all.css">
  <link rel="stylesheet" type="text/css" href="https://garrrychan.github.io/blog/theme/css/bootstrap.css">
  <link rel="stylesheet" type="text/css" href="https://garrrychan.github.io/blog/theme/css/pygments-monokai.css">
  <!--I need this for the logos-->
  <link href="https://maxcdn.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css" rel="stylesheet" integrity="sha384-wvfXpqpZZVQGK6TAh5PVlGOfQNHSoD2xbE+QkPxCAFlNEevoEH3Sl0sibVcOQVnN" crossorigin="anonymous">
<meta name="keywords" content="python">
</head>
<body>
  <div class="container">
    <div class="four columns sidebar">
<nav>
  <a href="https://garrrychan.github.io/blog/">
    <img src="https://garrrychan.github.io/blog/theme/images/logo.png" id="gravatar" alt="photo"/>
  </a>
  <h2><a href="https://garrrychan.github.io/blog/">Garry Chan</a></h2>

  <div id="bio">
    <p>I write about data science, and occasionally sports. Currently enrolled in a full time data science program. Former tech consultant, with a Bachelors in Mathematics.</p>
  </div>

  <div id="social">
    <a title="Garry on LinkedIn" href="https://www.linkedin.com/in/garrrychan">
      <i class="fa fa-linkedin"></i>
    </a>
    <a title="Garry on Github" href="https://github.com/garrrychan">
      <i class="fa fa-github"></i>
    </a>
  </div>

  <!---<div id="tags">
    <ul>
      </ul>
  </div>--->
</nav>    </div>

    <div class="eleven columns content">

<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML' async></script>
  <p class="meta">
    19 April 2019
    <a href="/">
      <i class="home fa fa-home"></i>
    </a>
  </p>

  <h1 class="title"><a href="https://garrrychan.github.io/blog/chocolate.html">Life is like a box of chocolates</a></h1>

  <div class="article_text" id="post">
    <p>Do you have a sweet tooth, a guilty pleasure, or something that you eat to make you feel <em>happy</em>? Given some attributes of a snack, can I classify your favourite snack as a chocolate, or not chocolate (i.e. candy)? </p>
<p>For example, if you told me that your favourite snack contains peanuts, and is not in a bar shape, my model should guess that this is chocolate (Reese Pieces)!</p>
<p>This is a classification problem, so let's spin up decision trees and sprinkle in some ensemble methods
(bagging &amp; boosting) to improve prediction performance, and add a kick of flavour. This is a fun exercise to explore a variety of algorithms!</p>
<hr>
<h3>Data</h3>
<p>Data is from <a href="https://github.com/fivethirtyeight/data/tree/master/candy-power-ranking">The Ultimate Halloween Candy Power Ranking</a>.</p>
<p><code>candy-data.csv</code> includes attributes for each snack. For binary variables, 1 means yes, 0 means no.</p>
<p>The data contains the following fields:</p>
<table class="table table-striped table-bordered">
<thead>
<tr>
<th>Feature</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>chocolate</td>
<td>Does it contain chocolate?</td>
</tr>
<tr>
<td>fruity</td>
<td>Is it fruit flavored?</td>
</tr>
<tr>
<td>caramel</td>
<td>Is there caramel in the candy?</td>
</tr>
<tr>
<td>peanutalmondy</td>
<td>Does it contain peanuts, peanut butter or almonds?</td>
</tr>
<tr>
<td>nougat</td>
<td>Does it contain nougat?</td>
</tr>
<tr>
<td>crispedricewafer</td>
<td>Does it contain crisped rice, wafers, or a cookie component?</td>
</tr>
<tr>
<td>hard</td>
<td>Is it a hard candy?</td>
</tr>
<tr>
<td>bar</td>
<td>Is it a candy bar?</td>
</tr>
<tr>
<td>pluribus</td>
<td>Is it one of many candies in a bag or box?</td>
</tr>
<tr>
<td>sugarpercent</td>
<td>The percentile of sugar it falls under within the data set.</td>
</tr>
<tr>
<td>pricepercent</td>
<td>The unit price percentile compared to the rest of the set.</td>
</tr>
</tbody>
</table>

<h4>Import libraries</h4>
<div class="highlight"><pre><span></span><span class="c1"># data wrangling</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="kn">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>

<span class="c1"># plotting &amp; viz</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="kn">as</span> <span class="nn">plt</span>
<span class="o">%</span><span class="n">matplotlib</span> <span class="n">inline</span>
<span class="o">%</span><span class="n">config</span> <span class="n">InlineBackend</span><span class="o">.</span><span class="n">figure_format</span> <span class="o">=</span> <span class="s1">&#39;svg&#39;</span>
<span class="n">plt</span><span class="o">.</span><span class="n">style</span><span class="o">.</span><span class="n">use</span><span class="p">(</span><span class="s1">&#39;fivethirtyeight&#39;</span><span class="p">)</span>
<span class="kn">import</span> <span class="nn">altair</span> <span class="kn">as</span> <span class="nn">alt</span> 
<span class="n">alt</span><span class="o">.</span><span class="n">renderers</span><span class="o">.</span><span class="n">enable</span><span class="p">(</span><span class="s1">&#39;notebook&#39;</span><span class="p">)</span>
<span class="kn">from</span> <span class="nn">IPython.display</span> <span class="kn">import</span> <span class="n">HTML</span>

<span class="c1"># tree visualization</span>
<span class="kn">from</span> <span class="nn">IPython.display</span> <span class="kn">import</span> <span class="n">Image</span>  
<span class="kn">import</span> <span class="nn">pydotplus</span>

<span class="c1"># supervised learning - modelling</span>
<span class="kn">from</span> <span class="nn">sklearn.tree</span> <span class="kn">import</span> <span class="n">DecisionTreeClassifier</span>
<span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">tree</span>
<span class="kn">import</span> <span class="nn">catboost</span> <span class="kn">as</span> <span class="nn">cb</span>
<span class="kn">from</span> <span class="nn">catboost</span> <span class="kn">import</span> <span class="n">cv</span><span class="p">,</span> <span class="n">CatBoostClassifier</span><span class="p">,</span> <span class="n">Pool</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">GridSearchCV</span> 

<span class="c1"># unsupervised learning</span>
<span class="kn">from</span> <span class="nn">sklearn.cluster</span> <span class="kn">import</span> <span class="n">KMeans</span>

<span class="c1"># model selection and evaluation</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span><span class="p">,</span> <span class="n">GridSearchCV</span><span class="p">,</span> <span class="n">cross_val_score</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">roc_auc_score</span><span class="p">,</span> <span class="n">accuracy_score</span><span class="p">,</span> <span class="n">make_scorer</span>

<span class="c1"># ensemble methods</span>
<span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">BaggingClassifier</span><span class="p">,</span> <span class="n">RandomForestClassifier</span>

<span class="c1"># support warnings</span>
<span class="kn">import</span> <span class="nn">warnings</span>
<span class="n">warnings</span><span class="o">.</span><span class="n">filterwarnings</span><span class="p">(</span><span class="n">action</span><span class="o">=</span><span class="s1">&#39;ignore&#39;</span><span class="p">,</span> <span class="n">category</span><span class="o">=</span><span class="ne">DeprecationWarning</span><span class="p">)</span>
</pre></div>


<div class="highlight"><pre><span></span><span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;./data/candy_data.csv&#39;</span><span class="p">)</span>
</pre></div>


<div class="highlight"><pre><span></span><span class="c1"># remove fake candies, One Dime, One Quarter</span>
<span class="n">df</span><span class="p">[(</span><span class="n">df</span><span class="o">.</span><span class="n">competitorname</span> <span class="o">==</span> <span class="s2">&quot;One dime&quot;</span><span class="p">)</span> <span class="o">|</span> <span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">competitorname</span> <span class="o">==</span> <span class="s2">&quot;One quarter&quot;</span><span class="p">)]</span> <span class="c1">#index 47,77</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">drop</span><span class="p">([</span><span class="mi">47</span><span class="p">,</span><span class="mi">77</span><span class="p">])</span><span class="o">.</span><span class="n">reset_index</span><span class="p">()</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">drop</span><span class="p">([</span><span class="s1">&#39;index&#39;</span><span class="p">,</span><span class="s1">&#39;winpercent&#39;</span><span class="p">],</span> <span class="mi">1</span><span class="p">)</span> <span class="c1"># drop win_%, focus on the snack attributes</span>
<span class="c1"># great, now we have 83 observations to practice models on</span>
<span class="n">HTML</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">head</span><span class="p">()</span><span class="o">.</span><span class="n">to_html</span><span class="p">(</span><span class="n">classes</span><span class="o">=</span><span class="s2">&quot;table table-striped table-bordered&quot;</span><span class="p">))</span>
</pre></div>


<table border="1" class="dataframe table table-striped table-bordered">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>competitorname</th>
      <th>chocolate</th>
      <th>fruity</th>
      <th>caramel</th>
      <th>peanutyalmondy</th>
      <th>nougat</th>
      <th>crispedricewafer</th>
      <th>hard</th>
      <th>bar</th>
      <th>pluribus</th>
      <th>sugarpercent</th>
      <th>pricepercent</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>Reeses Peanut Butter cup</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0.720</td>
      <td>0.651</td>
    </tr>
    <tr>
      <th>1</th>
      <td>Reeses Miniatures</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0.034</td>
      <td>0.279</td>
    </tr>
    <tr>
      <th>2</th>
      <td>Twix</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0.546</td>
      <td>0.906</td>
    </tr>
    <tr>
      <th>3</th>
      <td>Kit Kat</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0.313</td>
      <td>0.511</td>
    </tr>
    <tr>
      <th>4</th>
      <td>Snickers</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0.546</td>
      <td>0.651</td>
    </tr>
  </tbody>
</table>

<div class="highlight"><pre><span></span><span class="n">df</span><span class="o">.</span><span class="n">chocolate</span><span class="o">.</span><span class="n">value_counts</span><span class="p">()</span>
<span class="c1"># we have a pretty balanced class, between signal (#37 chocolates) and noise, and no missing values</span>
<span class="c1"># great, no imbalanced classes procedure, or imputation required</span>
</pre></div>


<div class="highlight"><pre><span></span>0    46
1    37
Name: chocolate, dtype: int64
</pre></div>


<div class="highlight"><pre><span></span><span class="c1"># target</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s2">&quot;chocolate&quot;</span><span class="p">]</span>

<span class="c1"># design matrix</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">loc</span><span class="p">[:,</span> <span class="s1">&#39;fruity&#39;</span><span class="p">:</span><span class="s1">&#39;pricepercent&#39;</span><span class="p">]</span>

<span class="c1"># train, test, split</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.25</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
</pre></div>


<div class="highlight"><pre><span></span><span class="c1"># baseline</span>
<span class="mi">1</span> <span class="o">-</span> <span class="n">y</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
</pre></div>


<div class="highlight"><pre><span></span>0.5542168674698795
</pre></div>


<p>Let's establish a 'naive model'. If we predicted everything as 100% chocolate, then we would be 55% accurate. This is our bench accuracy to beat.</p>
<hr>
<h3>Trees üå≤</h3>
<h4>Vanilla decision tree</h4>
<div class="highlight"><pre><span></span><span class="c1"># Instantiate </span>
<span class="n">tree_model</span> <span class="o">=</span> <span class="n">DecisionTreeClassifier</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>

<span class="c1"># Fit </span>
<span class="n">tree_model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span><span class="n">y_train</span><span class="p">);</span>
</pre></div>


<h4>Seeing is believing</h4>
<p>While trees are non parametric, one key benefit of decisions trees is that it's simple and interpretable with printouts. </p>
<div class="highlight"><pre><span></span><span class="n">dot_data</span> <span class="o">=</span> <span class="n">tree</span><span class="o">.</span><span class="n">export_graphviz</span><span class="p">(</span>
    <span class="n">tree_model</span><span class="p">,</span> 
    <span class="n">out_file</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> 
    <span class="n">feature_names</span><span class="o">=</span><span class="n">X</span><span class="o">.</span><span class="n">columns</span><span class="p">,</span>  
    <span class="n">class_names</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;chocolate&quot;</span><span class="p">,</span><span class="s2">&quot;candy&quot;</span><span class="p">]</span> <span class="c1">#1 -&gt; chocolate, 0 -&gt; candy</span>
<span class="p">)</span>

<span class="n">graph</span> <span class="o">=</span> <span class="n">pydotplus</span><span class="o">.</span><span class="n">graph_from_dot_data</span><span class="p">(</span><span class="n">dot_data</span><span class="p">)</span>  
<span class="c1"># Image(graph.create_png())</span>
<span class="c1"># graph.write_svg(&quot;candytree.svg&quot;);</span>
</pre></div>


<p><img src="images/candytree.svg" alt="candytree" height="860" width="758"></p>
<p>First, the tree is split on fruit flavour. If it's fruity, then it's probably not chocolate. Makes sense. </p>
<p>And, if the snack is singular (i.e. 'pluribus'), contains peanuts, is high in sugar, and has nougats, then it's probably chocolate.</p>
<hr>
<ul>
<li>
<p>By default, the tree is split at each node based on Gini impurity. Gini is a measure of variance across the K classes; the smaller the Gini index, the better.</p>
</li>
<li>
<p>Remember, for a binary class, gini = 0 is a 'pure' split representing perfect equality, whereas gini = 0.5 is not pure.</p>
</li>
<li>
<p>You may see some splits where the nodes are both predicted to be chocolate. Well, why is a split event performed? Well, the split occurred because it increased node purity. That is the left split has 2/2 observations with a response value of chocolate, where as the right split is 1/2. If we have a new observation that belongs to the region in the left leaf, we can be sure its response value is chocolate, in contrast, if it goes into the right leaf, than it is probably yes, but we are much less certain.</p>
</li>
</ul>
<p>Now, let's use our model to predict if Reese's and Fuzzy Peaches are chocolate or not.</p>
<div class="highlight"><pre><span></span><span class="c1"># X.iloc[0:1,:].to_dict(orient=&quot;list&quot;)</span>

<span class="n">reeses</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;fruity&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">0</span><span class="p">],</span>
 <span class="s1">&#39;caramel&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">0</span><span class="p">],</span>
 <span class="s1">&#39;peanutyalmondy&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">1</span><span class="p">],</span>
 <span class="s1">&#39;nougat&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">0</span><span class="p">],</span>
 <span class="s1">&#39;crispedricewafer&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">0</span><span class="p">],</span>
 <span class="s1">&#39;hard&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">0</span><span class="p">],</span>
 <span class="s1">&#39;bar&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">0</span><span class="p">],</span>
 <span class="s1">&#39;pluribus&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">0</span><span class="p">],</span>
 <span class="s1">&#39;sugarpercent&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mf">0.72000003</span><span class="p">],</span>
 <span class="s1">&#39;pricepercent&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mf">0.65100002</span><span class="p">]}</span>

<span class="c1"># new observation </span>
<span class="n">fuzzy_peach</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;fruity&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">1</span><span class="p">],</span>
 <span class="s1">&#39;caramel&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">0</span><span class="p">],</span>
 <span class="s1">&#39;peanutyalmondy&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">0</span><span class="p">],</span>
 <span class="s1">&#39;nougat&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">0</span><span class="p">],</span>
 <span class="s1">&#39;crispedricewafer&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">0</span><span class="p">],</span>
 <span class="s1">&#39;hard&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">0</span><span class="p">],</span>
 <span class="s1">&#39;bar&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">0</span><span class="p">],</span>
 <span class="s1">&#39;pluribus&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">0</span><span class="p">],</span>
 <span class="s1">&#39;sugarpercent&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mf">0.75</span><span class="p">],</span>
 <span class="s1">&#39;pricepercent&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mf">0.5</span><span class="p">]}</span>

<span class="n">reeses</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">reeses</span><span class="p">)</span>
<span class="n">fuzzy_peach</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">fuzzy_peach</span><span class="p">)</span>
</pre></div>


<div class="highlight"><pre><span></span><span class="k">print</span><span class="p">(</span><span class="n">f</span><span class="s1">&#39; It</span><span class="se">\&#39;</span><span class="s1">s chocolate! {tree_model.predict(reeses)}&#39;</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">f</span><span class="s1">&#39; It</span><span class="se">\&#39;</span><span class="s1">s candy! {tree_model.predict(fuzzy_peach)}&#39;</span><span class="p">)</span>
</pre></div>


<div class="highlight"><pre><span></span> It&#39;s chocolate! [1]
 It&#39;s candy! [0]
</pre></div>


<div class="highlight"><pre><span></span><span class="c1"># Score: By default, cross_val_score calculates accuracy for trees</span>
<span class="k">print</span><span class="p">(</span><span class="n">f</span><span class="s1">&#39; tree_model train accuracy: {cross_val_score(tree_model, X_train, y_train, cv=5, scoring=make_scorer(accuracy_score)).mean()}&#39;</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">f</span><span class="s1">&#39; tree_model test accuracy: {cross_val_score(tree_model, X_test, y_test, cv=5).mean()}&#39;</span><span class="p">)</span>
</pre></div>


<div class="highlight"><pre><span></span> tree_model train accuracy: 0.7931235431235432
 tree_model test accuracy: 0.7366666666666667
</pre></div>


<p>A vanilla tree performs okay, as it's able to classify 74% of our test observations correctly, but there is some overfitting. </p>
<p>It is significantly better than a naive model, so it is picking up signal. This is our new benchmark to beat. By applying bagging, let's see if we can cut down the variance.</p>
<hr>
<h3>Ensemble methods</h3>
<h4>Bagging</h4>
<p>Bagging stands for bootstrap aggregating. </p>
<p>By definition, it's an averaging method and the idea is to build several bootstrapped estimators (i.e. trees) independently and then to average their predictions. The combined estimator is usually better than any of the single base estimator because its variance should be reduced.</p>
<p>Usually, bagging mitigates overfitting by exposing different trees to different sub-samples of the training set.</p>
<div class="highlight"><pre><span></span><span class="c1"># Set of bagged decision trees</span>

<span class="c1"># Instantiate</span>
<span class="n">bag</span> <span class="o">=</span> <span class="n">BaggingClassifier</span><span class="p">(</span><span class="n">n_estimators</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">max_samples</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">random_state</span> <span class="o">=</span> <span class="mi">42</span><span class="p">)</span>
<span class="c1"># &#39;n_estimators=10&#39;, -&gt; 10 trees in the ensemble to be averaged</span>
<span class="c1"># &#39;max_samples=1.0&#39;, -&gt; 1 samples to draw from X to train each tree</span>

<span class="c1"># Fit</span>
<span class="n">bag</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="c1"># Predict</span>
<span class="k">print</span><span class="p">(</span><span class="n">f</span><span class="s1">&#39; bagged decision tree train accuracy {cross_val_score(bag, X_train, y_train, cv=5).mean()}&#39;</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">f</span><span class="s1">&#39; bagged decision tree test accuracy {cross_val_score(bag, X_test, y_test, cv=5).mean()}&#39;</span><span class="p">)</span>
</pre></div>


<div class="highlight"><pre><span></span> bagged decision tree train accuracy 0.8367132867132867
 bagged decision tree test accuracy 0.7366666666666667
</pre></div>


<p>As we increased the <code>n_estimators</code>, the test accuracy improved, but it's capped at about 74%. This did not improve over our vanilla tree on the test set. Perhaps, we are still averaging highly correlated trees, therefore we are not enjoying the benefits of bagging.</p>
<p>Suppose that there is one very strong predictor, along with a number of other moderately strong predictors. Then in the collection of bagged trees, most or all of the trees will use this strong predictor in the top split (i.e. fruity). Consequently, all of the bagged trees will look quite similar to each other. </p>
<p>Hence, the predictions from the bagged trees will be highly correlated -&gt; averaging many highly correlated quantities does not lead to as large of a reduction in variance -&gt; bagging will not lead to a substantial reduction in variance over a single tree in this setting.</p>
<hr>
<h4>Random Forest üå≥üå≥üå≥</h4>
<p>Let's not be so greedy and gluttonous.</p>
<p>Vanilla decisions trees and bagging uses a top-down, greedy approach. It is greedy because at each step of the tree-building process, the best split is made at that particular step, rather than looking ahead and picking a split that will lead to a better tree in some future step.</p>
<p>Random forests provide an improvement over bagged trees by way of a random small tweak that decorrelates the trees. Each time a split in a tree is considered, a random sample of m&lt;p predictors is chosen as split candidates from the full set of p predictors. </p>
<p>The split is allowed to use only one of those m predictors, and this ultimately helps decorrelates the trees and generally leads to more accurate predictions. </p>
<div class="highlight"><pre><span></span><span class="c1">#  random forest</span>
<span class="n">random_forest_model</span> <span class="o">=</span> <span class="n">RandomForestClassifier</span><span class="p">(</span><span class="n">n_estimators</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>

<span class="k">print</span><span class="p">(</span><span class="n">cross_val_score</span><span class="p">(</span><span class="n">random_forest_model</span><span class="p">,</span> <span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">())</span>
<span class="k">print</span><span class="p">(</span><span class="n">cross_val_score</span><span class="p">(</span><span class="n">random_forest_model</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">())</span>
</pre></div>


<div class="highlight"><pre><span></span>0.8564102564102564
0.8033333333333333
</pre></div>


<p>Random forests further improved the accuracy on the test set to 80%, and there is less overfitting now compared to bagging. The randomness introduced, also decreased the variance.</p>
<p>Let's fine tune our hyperparameters to optimize the performance.</p>
<div class="highlight"><pre><span></span><span class="c1"># Grid Search over n estimators, max samples</span>
<span class="n">params</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s1">&#39;n_estimators&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">5</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">15</span><span class="p">],</span> 
    <span class="s1">&#39;max_depth&#39;</span><span class="p">:</span> <span class="p">[</span><span class="bp">None</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> 
    <span class="s1">&#39;min_samples_split&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">]</span>
<span class="p">}</span>

<span class="c1"># instantiate </span>
<span class="n">rf_grid_search</span> <span class="o">=</span> <span class="n">GridSearchCV</span><span class="p">(</span><span class="n">random_forest_model</span><span class="p">,</span> <span class="n">param_grid</span><span class="o">=</span><span class="n">params</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
<span class="n">rf_grid_search</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="c1"># up to 90% on train, increased with more trees but not too many</span>
<span class="c1"># but doesn&#39;t really matter as larger number of trees do not overfit as you are averaging</span>
<span class="c1"># 86% accuracy on test!</span>
<span class="k">print</span><span class="p">(</span><span class="n">rf_grid_search</span><span class="o">.</span><span class="n">best_score_</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">rf_grid_search</span><span class="o">.</span><span class="n">best_params_</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">rf_grid_search</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span><span class="n">y_test</span><span class="p">))</span>
</pre></div>


<div class="highlight"><pre><span></span>0.9032258064516129
{&#39;max_depth&#39;: None, &#39;min_samples_split&#39;: 4, &#39;n_estimators&#39;: 10}
0.8571428571428571
</pre></div>


<hr>
<h4>Boosting</h4>
<p>Let's introduce another ensemble method. </p>
<p>With boosting methods, trees are grown sequentially. Each tree is grown using information from previously grown trees. Boosting does not involve bootstrap sampling; instead each tree is fit on a modified version of the original data set. Similarly, the final prediction is constructed by a weighted vote: Weights for each base model depend on their training errors or misclassification rates.</p>
<p>This method focuses on reduce bias by taking weak learners to start, and the algorithm is penalize wrong predictions in an iterative nature. Note, the number of trees is important here, as boosting can overfit, unlike in bagging and random forests. We can use cross validation to choose <code>n_estimators</code>. Boosting focuses on reducing bias, but can also reduce variance.<br></p>
<h5>CatBoost  üê±</h5>
<p>Do cats like chocolate? I don't know. Let's see if this boosting model can really purr, by implementing Cat Boost. </p>
<p><a href="https://github.com/catboost/catboost">CatBoost</a> is a flavour (are you sick of my food puns yet), of boosting. It is a fast, scalable, high performance gradient boosting on decision trees library, used for ranking, classification, regression and other machine learning tasks.</p>
<p>This is a pretty small data set, so we won't really see it shine like with large data sets, but it does come with some neat graphics for model evaluation metrics, which I did want to highlight.</p>
<div class="highlight"><pre><span></span><span class="c1"># Instantiate</span>
<span class="n">cat_model</span> <span class="o">=</span> <span class="n">cb</span><span class="o">.</span><span class="n">CatBoostClassifier</span><span class="p">(</span>
    <span class="n">iterations</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> 
    <span class="n">early_stopping_rounds</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="c1"># stop after 10 rounds if no improvement</span>
    <span class="n">custom_loss</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;AUC&#39;</span><span class="p">,</span> <span class="s1">&#39;Accuracy&#39;</span><span class="p">],</span> <span class="c1"># captures logloss by default, also capture auc and accuracy</span>
    <span class="n">random_state</span> <span class="o">=</span> <span class="mi">42</span>
<span class="p">)</span>

<span class="c1"># fit</span>
<span class="n">cat_model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">eval_set</span><span class="o">=</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">),</span><span class="n">verbose</span><span class="o">=</span><span class="bp">False</span><span class="p">);</span>
</pre></div>


<div class="highlight"><pre><span></span><span class="c1"># categorical data</span>
<span class="c1"># indices 0 to 7 </span>
<span class="n">categorical_features_indices</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>

<span class="n">cv_params</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;early_stopping_rounds&#39;</span><span class="p">:</span> <span class="mi">10</span><span class="p">,</span>
 <span class="s1">&#39;random_state&#39;</span><span class="p">:</span> <span class="mi">42</span><span class="p">,</span>
 <span class="s1">&#39;custom_loss&#39;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;AUC&#39;</span><span class="p">,</span> <span class="s1">&#39;Accuracy&#39;</span><span class="p">],</span>
 <span class="s1">&#39;loss_function&#39;</span><span class="p">:</span> <span class="s1">&#39;Logloss&#39;</span><span class="p">,</span>
 <span class="s1">&#39;iterations&#39;</span><span class="p">:</span> <span class="mi">100</span><span class="p">}</span>
</pre></div>


<div class="highlight"><pre><span></span><span class="n">cv_data</span> <span class="o">=</span> <span class="n">cv</span><span class="p">(</span>
    <span class="n">Pool</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">cat_features</span><span class="o">=</span><span class="n">categorical_features_indices</span><span class="p">),</span>
    <span class="n">cv_params</span><span class="p">,</span>
    <span class="n">fold_count</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span>
    <span class="n">verbose</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
<span class="c1">#    plot=True</span>
<span class="c1"># 55 iterations for it to stabilize, 5 folds cv</span>
</pre></div>


<p><img src="images/catboost.png" alt="catboost" height="364" width="783"></p>
<div class="highlight"><pre><span></span><span class="n">best_value</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="n">cv_data</span><span class="p">[</span><span class="s1">&#39;test-Logloss-mean&#39;</span><span class="p">])</span>
<span class="n">best_iter</span> <span class="o">=</span> <span class="n">cv_data</span><span class="p">[</span><span class="s1">&#39;test-Logloss-mean&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">idxmin</span><span class="p">()</span>

<span class="k">print</span><span class="p">(</span><span class="s1">&#39;Best validation Logloss score, {:.4f}¬±{:.4f} on step {}&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
    <span class="n">best_value</span><span class="p">,</span>
    <span class="n">cv_data</span><span class="p">[</span><span class="s1">&#39;test-Logloss-std&#39;</span><span class="p">][</span><span class="n">best_iter</span><span class="p">],</span>
    <span class="n">best_iter</span><span class="p">)</span>
<span class="p">)</span>
</pre></div>


<div class="highlight"><pre><span></span>Best validation Logloss score, 0.3301¬±0.1217 on step 45
</pre></div>


<div class="highlight"><pre><span></span><span class="k">print</span><span class="p">(</span><span class="s1">&#39;Validation accuracy score, {:.4f} on step {}&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
    <span class="n">cv_data</span><span class="p">[</span><span class="n">cv_data</span><span class="p">[</span><span class="s2">&quot;iterations&quot;</span><span class="p">]</span><span class="o">==</span><span class="mi">45</span><span class="p">][</span><span class="s2">&quot;test-Accuracy-mean&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">to_numpy</span><span class="p">()[</span><span class="mi">0</span><span class="p">],</span>
    <span class="n">best_iter</span><span class="p">))</span>
</pre></div>


<div class="highlight"><pre><span></span>Validation accuracy score, 0.8779 on step 45
</pre></div>


<p>Aside: By default, CatBoost optimizes for minimizing logloss, and with cross validation we select the model at 45 iterations. Basically, logloss increases as the predicted probability diverges from the actual label.</p>
<table class="table">
<thead class="table-striped">
<tr>
<th>Test scores</th>
<th>Naive</th>
<th>Vanilla Decision Tree</th>
<th>Bagging</th>
<th>Random Forest</th>    
<th>CatBoost</th>
</tr>
<tbody>
<tr>
<td>Accuracy</td>
<td>55.4%</td>
<td>73.7%</td>
<td>73.7%</td>
<td>85.7%</td>
<td>87.8%</td>
</tr>
</tbody>
</table>

<p>As a caveat, with ensemble methods it is not always apparent which variables are most important to the procedure. Generally, boosting and bagging methods improves prediction accuracy at the expense of interpretability. Luckily, CatBoost has a<code>feature_importances</code> method to understand which feature made the greatest contribution to the final result. </p>
<p>This shows that features <code>fruity</code> and <code>pricepercent</code> had the biggest influence on classifying if something was a chocolate or a candy. The same as our vanilla tree print out.</p>
<div class="highlight"><pre><span></span><span class="n">cat_model</span><span class="o">.</span><span class="n">get_feature_importance</span><span class="p">(</span><span class="n">prettified</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
</pre></div>


<div class="highlight"><pre><span></span>[(&#39;fruity&#39;, 62.812208418548465),
 (&#39;pricepercent&#39;, 21.388700907444946),
 (&#39;pluribus&#39;, 4.915296243219509),
 (&#39;sugarpercent&#39;, 3.809671603132506),
 (&#39;caramel&#39;, 2.8654253284742484),
 (&#39;nougat&#39;, 1.620219841699855),
 (&#39;bar&#39;, 1.4484940632542251),
 (&#39;hard&#39;, 1.0595256574622567),
 (&#39;peanutyalmondy&#39;, 0.06755089925898686),
 (&#39;crispedricewafer&#39;, 0.012907037505001707)]
</pre></div>


<hr>
<h3>Bonus: Unsupervised learning</h3>
<p>For a taste of what's to come in future posts, let's apply unsupervised learning to a problem to identify targets labels, y. No labels are given to the learning algorithm; it's left to find structure in its input. Unsupervised learning can be a goal in itself (discovering hidden patterns in data), or a means towards an end (feature learning). This is the latter application.</p>
<p>Suppose you're a manager at Cadbury, and you want to create a new confectionery product for the masses. However, all you know is certain attributes of your snacks, and want to identify if there structure in your inventory. If there are patterns, then you might be able to create a new product with attributes that people will enjoy!</p>
<hr>
<h4>Life is like a 'cluster' of chocolates - Forrest Gump</h4>
<p><img src="images/gump.png" alt="gump" height="486" width="864"></p>
<p>Okay, let's pretend we only have X. Our previous supervised learning model wouldn't be able to learn, and predict if something was chocolate or candy.</p>
<p>How might we cluster these snacks into different categories? </p>
<p>We can use K-means clustering to solve this, it's one of the most common methods. Again, we are NOT classifying explicitly, but rather exploring if there are common characteristics in k groups.</p>
<div class="highlight"><pre><span></span><span class="n">X</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">loc</span><span class="p">[:,</span> <span class="s1">&#39;fruity&#39;</span><span class="p">:</span><span class="s1">&#39;pricepercent&#39;</span><span class="p">]</span>
<span class="n">HTML</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span><span class="o">.</span><span class="n">to_html</span><span class="p">(</span><span class="n">classes</span><span class="o">=</span><span class="s2">&quot;table table-striped table-bordered&quot;</span><span class="p">))</span>
</pre></div>


<table border="1" class="dataframe table table-striped table-bordered">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>fruity</th>
      <th>caramel</th>
      <th>peanutyalmondy</th>
      <th>nougat</th>
      <th>crispedricewafer</th>
      <th>hard</th>
      <th>bar</th>
      <th>pluribus</th>
      <th>sugarpercent</th>
      <th>pricepercent</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>60</th>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0.22</td>
      <td>0.325</td>
    </tr>
  </tbody>
</table>

<p>First, let's look at the ground truth. </p>
<div class="highlight"><pre><span></span><span class="n">df</span><span class="p">[</span><span class="s1">&#39;emoji&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="p">[{</span><span class="mi">1</span><span class="p">:</span> <span class="s1">&#39;üç´&#39;</span><span class="p">,</span> <span class="mi">0</span><span class="p">:</span> <span class="s1">&#39;üç¨&#39;</span><span class="p">}[</span><span class="n">snack</span><span class="p">]</span> <span class="k">for</span> <span class="n">snack</span> <span class="ow">in</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;chocolate&#39;</span><span class="p">]]</span>
<span class="n">HTML</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">head</span><span class="p">()</span><span class="o">.</span><span class="n">to_html</span><span class="p">(</span><span class="n">classes</span><span class="o">=</span><span class="s2">&quot;table table-striped table-bordered&quot;</span><span class="p">))</span>
</pre></div>


<table border="1" class="dataframe table table-striped table-bordered">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>competitorname</th>
      <th>chocolate</th>
      <th>fruity</th>
      <th>caramel</th>
      <th>peanutyalmondy</th>
      <th>nougat</th>
      <th>crispedricewafer</th>
      <th>hard</th>
      <th>bar</th>
      <th>pluribus</th>
      <th>sugarpercent</th>
      <th>pricepercent</th>
      <th>emoji</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>Reeses Peanut Butter cup</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0.720</td>
      <td>0.651</td>
      <td>üç´</td>
    </tr>
    <tr>
      <th>1</th>
      <td>Reeses Miniatures</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0.034</td>
      <td>0.279</td>
      <td>üç´</td>
    </tr>
    <tr>
      <th>2</th>
      <td>Twix</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0.546</td>
      <td>0.906</td>
      <td>üç´</td>
    </tr>
    <tr>
      <th>3</th>
      <td>Kit Kat</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0.313</td>
      <td>0.511</td>
      <td>üç´</td>
    </tr>
    <tr>
      <th>4</th>
      <td>Snickers</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0.546</td>
      <td>0.651</td>
      <td>üç´</td>
    </tr>
  </tbody>
</table>

<div class="highlight"><pre><span></span><span class="n">ground_truth</span> <span class="o">=</span> <span class="p">(</span><span class="n">alt</span><span class="o">.</span><span class="n">Chart</span><span class="p">(</span><span class="n">df</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s1">&#39;Ground Truth&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">mark_text</span><span class="p">(</span><span class="n">filled</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span>
    <span class="n">x</span><span class="o">=</span><span class="n">alt</span><span class="o">.</span><span class="n">X</span><span class="p">(</span><span class="s1">&#39;sugarpercent&#39;</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="n">alt</span><span class="o">.</span><span class="n">Axis</span><span class="p">(</span><span class="n">title</span><span class="o">=</span><span class="s1">&#39;sugar percentile&#39;</span><span class="p">,</span> <span class="n">values</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mf">0.2</span><span class="p">,</span><span class="mf">0.4</span><span class="p">,</span><span class="mf">0.6</span><span class="p">,</span><span class="mf">0.8</span><span class="p">,</span><span class="mi">1</span><span class="p">])),</span>
    <span class="n">y</span><span class="o">=</span><span class="n">alt</span><span class="o">.</span><span class="n">X</span><span class="p">(</span><span class="s1">&#39;pricepercent&#39;</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="n">alt</span><span class="o">.</span><span class="n">Axis</span><span class="p">(</span><span class="n">title</span><span class="o">=</span><span class="s1">&#39;price percentile&#39;</span><span class="p">,</span> <span class="n">values</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mf">0.2</span><span class="p">,</span><span class="mf">0.4</span><span class="p">,</span><span class="mf">0.6</span><span class="p">,</span><span class="mf">0.8</span><span class="p">,</span><span class="mi">1</span><span class="p">])),</span>
    <span class="n">text</span><span class="o">=</span><span class="s1">&#39;emoji&#39;</span><span class="p">))</span><span class="c1">#.save(&#39;../content/images/ground_truth.svg&#39;)</span>
</pre></div>


<p><img alt="ground_truth.svg" src="images/ground_truth.svg"></p>
<p>While it's not a very strong cluster, you can see that chocolates (black) tend to cost more relative to candies (red). The spread of the sugar amount varies, you have very sweet and less sweet snacks.</p>
<p>Let's use k-means clustering, with k=2 clusters to see if we can categorize these snacks into two groups. </p>
<div class="highlight"><pre><span></span><span class="c1"># instantiate</span>
<span class="n">kmeans_model</span> <span class="o">=</span> <span class="n">KMeans</span><span class="p">(</span><span class="n">n_clusters</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span><span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>

<span class="c1"># fit on X, don&#39;t need train and test as there&#39;s no &#39;accuracy&#39; to measure</span>
<span class="c1"># there is inertia, and silhouette, for comparing clustering methods, out of scope for this post</span>
<span class="n">kmeans_model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

<span class="c1"># predict labels, group everything into 1 or 0</span>
<span class="n">kmeans_model</span><span class="o">.</span><span class="n">labels_</span><span class="p">;</span>

<span class="c1"># add the learned cluster labels back to df</span>
<span class="n">df</span><span class="p">[</span><span class="s2">&quot;cluster_label&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">kmeans_model</span><span class="o">.</span><span class="n">labels_</span>
</pre></div>


<p>Now, each snack has a label of 1 or 0. From these 10 dimensions (fruity, caramel ... price percentile), the model was able to group these into two clusters that are the most similar to each other, by minimizing the within-cluster sum of squares.</p>
<p>Since labels are arbitrary when clustering, we can rename the cluster labels to chocolate and candy emojis so we can compare to ground truth (since we have it, generally we wouldn't).</p>
<div class="highlight"><pre><span></span><span class="n">df</span><span class="p">[</span><span class="s2">&quot;kmeans_emoji&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;cluster_label&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">map</span><span class="p">({</span><span class="mi">1</span><span class="p">:</span> <span class="s2">&quot;üç´&quot;</span> <span class="p">,</span> <span class="mi">0</span><span class="p">:</span> <span class="s2">&quot;üç¨&quot;</span><span class="p">})</span>
<span class="n">HTML</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">head</span><span class="p">()</span><span class="o">.</span><span class="n">to_html</span><span class="p">(</span><span class="n">classes</span><span class="o">=</span><span class="s2">&quot;table table-striped table-bordered&quot;</span><span class="p">))</span>
</pre></div>


<table border="1" class="dataframe table table-striped table-bordered">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>competitorname</th>
      <th>chocolate</th>
      <th>fruity</th>
      <th>caramel</th>
      <th>peanutyalmondy</th>
      <th>nougat</th>
      <th>crispedricewafer</th>
      <th>hard</th>
      <th>bar</th>
      <th>pluribus</th>
      <th>sugarpercent</th>
      <th>pricepercent</th>
      <th>emoji</th>
      <th>cluster_label</th>
      <th>kmeans_emoji</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>Reeses Peanut Butter cup</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0.720</td>
      <td>0.651</td>
      <td>üç´</td>
      <td>1</td>
      <td>üç´</td>
    </tr>
    <tr>
      <th>1</th>
      <td>Reeses Miniatures</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0.034</td>
      <td>0.279</td>
      <td>üç´</td>
      <td>1</td>
      <td>üç´</td>
    </tr>
    <tr>
      <th>2</th>
      <td>Twix</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0.546</td>
      <td>0.906</td>
      <td>üç´</td>
      <td>1</td>
      <td>üç´</td>
    </tr>
    <tr>
      <th>3</th>
      <td>Kit Kat</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0.313</td>
      <td>0.511</td>
      <td>üç´</td>
      <td>1</td>
      <td>üç´</td>
    </tr>
    <tr>
      <th>4</th>
      <td>Snickers</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0.546</td>
      <td>0.651</td>
      <td>üç´</td>
      <td>1</td>
      <td>üç´</td>
    </tr>
  </tbody>
</table>

<div class="highlight"><pre><span></span><span class="n">accuracy_score</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s2">&quot;emoji&quot;</span><span class="p">],</span><span class="n">df</span><span class="p">[</span><span class="s2">&quot;kmeans_emoji&quot;</span><span class="p">])</span>
</pre></div>


<div class="highlight"><pre><span></span>0.8072289156626506
</pre></div>


<p>By grouping observations into two sets, one group looks like chocolate, and the other group looks like candy, when compared on the two dimensions of price and sugar percentile.</p>
<p>67 out of 83 observations were classified correctly. Hey, that's pretty good! </p>
<p>Now, let's plot this with the centroids.</p>
<div class="highlight"><pre><span></span><span class="n">kmeans_model</span><span class="o">.</span><span class="n">cluster_centers_</span> <span class="c1">#2 clusters in 11 dimensions</span>
<span class="c1"># take the elements for sugar and price</span>
<span class="c1"># index 9 and 10</span>
<span class="n">kmeans_model</span><span class="o">.</span><span class="n">cluster_centers_</span><span class="p">[:,</span><span class="mi">8</span><span class="p">:]</span>
</pre></div>


<div class="highlight"><pre><span></span>array([[0.48074999, 0.38033928],
       [0.50892592, 0.66403704]])
</pre></div>


<div class="highlight"><pre><span></span><span class="c1"># plot centroids</span>
<span class="n">centroids</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">kmeans_model</span><span class="o">.</span><span class="n">cluster_centers_</span><span class="p">[:,</span><span class="mi">8</span><span class="p">:])</span>
<span class="n">centroids</span><span class="o">.</span><span class="n">columns</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;sugarpercent&quot;</span><span class="p">,</span><span class="s2">&quot;pricepercent&quot;</span><span class="p">]</span>
<span class="n">centroids</span><span class="p">[</span><span class="s1">&#39;kmeans_emoji&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">({</span><span class="s2">&quot;kmeans_emoji&quot;</span><span class="p">:</span> <span class="p">[</span><span class="s2">&quot;üç¨&quot;</span><span class="p">,</span><span class="s2">&quot;üç´&quot;</span><span class="p">]})</span>
</pre></div>


<div class="highlight"><pre><span></span><span class="n">kmeans</span> <span class="o">=</span> <span class="p">(</span><span class="n">alt</span><span class="o">.</span><span class="n">Chart</span><span class="p">(</span><span class="n">df</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s1">&#39;K-means, with 2 Clusters&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">mark_text</span><span class="p">(</span><span class="n">filled</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span>
    <span class="n">x</span><span class="o">=</span><span class="n">alt</span><span class="o">.</span><span class="n">X</span><span class="p">(</span><span class="s1">&#39;sugarpercent&#39;</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="n">alt</span><span class="o">.</span><span class="n">Axis</span><span class="p">(</span><span class="n">title</span><span class="o">=</span><span class="s1">&#39;sugar percentile&#39;</span><span class="p">,</span> <span class="n">values</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mf">0.2</span><span class="p">,</span><span class="mf">0.4</span><span class="p">,</span><span class="mf">0.6</span><span class="p">,</span><span class="mf">0.8</span><span class="p">,</span><span class="mi">1</span><span class="p">])),</span>
    <span class="n">y</span><span class="o">=</span><span class="n">alt</span><span class="o">.</span><span class="n">X</span><span class="p">(</span><span class="s1">&#39;pricepercent&#39;</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="n">alt</span><span class="o">.</span><span class="n">Axis</span><span class="p">(</span><span class="n">title</span><span class="o">=</span><span class="s1">&#39;price percentile&#39;</span><span class="p">,</span> <span class="n">values</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mf">0.2</span><span class="p">,</span><span class="mf">0.4</span><span class="p">,</span><span class="mf">0.6</span><span class="p">,</span><span class="mf">0.8</span><span class="p">,</span><span class="mi">1</span><span class="p">])),</span>
    <span class="n">text</span><span class="o">=</span><span class="s1">&#39;kmeans_emoji&#39;</span><span class="p">))</span>

<span class="n">centroid_plot</span> <span class="o">=</span> <span class="p">(</span><span class="n">alt</span><span class="o">.</span><span class="n">Chart</span><span class="p">(</span><span class="n">centroids</span><span class="p">)</span><span class="o">.</span><span class="n">mark_text</span><span class="p">(</span><span class="n">filled</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">40</span><span class="p">)</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span>
    <span class="n">x</span><span class="o">=</span><span class="n">alt</span><span class="o">.</span><span class="n">X</span><span class="p">(</span><span class="s1">&#39;sugarpercent&#39;</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="n">alt</span><span class="o">.</span><span class="n">Axis</span><span class="p">(</span><span class="n">title</span><span class="o">=</span><span class="s1">&#39;sugar percentile&#39;</span><span class="p">,</span> <span class="n">values</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mf">0.2</span><span class="p">,</span><span class="mf">0.4</span><span class="p">,</span><span class="mf">0.6</span><span class="p">,</span><span class="mf">0.8</span><span class="p">,</span><span class="mi">1</span><span class="p">])),</span>
    <span class="n">y</span><span class="o">=</span><span class="n">alt</span><span class="o">.</span><span class="n">X</span><span class="p">(</span><span class="s1">&#39;pricepercent&#39;</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="n">alt</span><span class="o">.</span><span class="n">Axis</span><span class="p">(</span><span class="n">title</span><span class="o">=</span><span class="s1">&#39;price percentile&#39;</span><span class="p">,</span> <span class="n">values</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mf">0.2</span><span class="p">,</span><span class="mf">0.4</span><span class="p">,</span><span class="mf">0.6</span><span class="p">,</span><span class="mf">0.8</span><span class="p">,</span><span class="mi">1</span><span class="p">])),</span>
    <span class="n">text</span><span class="o">=</span><span class="s1">&#39;kmeans_emoji&#39;</span><span class="p">))</span>
</pre></div>


<div class="highlight"><pre><span></span><span class="p">(</span><span class="n">alt</span><span class="o">.</span><span class="n">hconcat</span><span class="p">(</span><span class="n">kmeans</span> <span class="o">+</span> <span class="n">centroid_plot</span><span class="p">,</span> <span class="n">ground_truth</span><span class="p">)</span><span class="o">.</span><span class="n">configure_axis</span><span class="p">(</span><span class="n">gridOpacity</span><span class="o">=</span><span class="mf">0.25</span><span class="p">))</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="s1">&#39;../content/images/compare.svg&#39;</span><span class="p">)</span>
</pre></div>


<p><img alt="compare.svg" src="images/compare.svg"></p>
<p>Note: For the k-means graph on the left, the 2 clusters centroids (calculated from multi-dimensional space) are indicated by the oversized chocolate and candy emojis. They are computed as the mean of the observations assigned to each cluster.</p>
  </div>



      <div class="footer">
<div class="disclaimer">

    <p>
      ¬© Garry Chan,  &mdash; built with <a href="http://getpelican.com" target="_blank">Pelican</a>, using a port of jekyll theme <a href="https://github.com/swanson/lagom" target="_blank">Lagom</a>.
    </p>
  </div>      </div>
    </div>
  </div>

</body>
</html>